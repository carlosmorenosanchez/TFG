{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jugando asi es imposible que ganemos un partido…</td>\n",
       "      <td>Tristeza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Que venga Arthur me tiene más emocionado que c...</td>\n",
       "      <td>Felicidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Me ha gustado un vídeo de @YouTube (https://t....</td>\n",
       "      <td>Felicidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Deportes ➡️ Hace 17 años, Messi por primera v...</td>\n",
       "      <td>Felicidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Desde la dirección hasta los actores el vídeo ...</td>\n",
       "      <td>Felicidad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet       Info\n",
       "0   Jugando asi es imposible que ganemos un partido…   Tristeza\n",
       "1  Que venga Arthur me tiene más emocionado que c...  Felicidad\n",
       "2  Me ha gustado un vídeo de @YouTube (https://t....  Felicidad\n",
       "3  #Deportes ➡️ Hace 17 años, Messi por primera v...  Felicidad\n",
       "4  Desde la dirección hasta los actores el vídeo ...  Felicidad"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The files are coded in ISO-8859-1\n",
    "\n",
    "df = pd.read_csv(\"data/Felicidad-Tristeza.csv\")\n",
    "df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and Y\n",
    "X = df['Tweet'].values.astype(str)\n",
    "y = df['Info'].values.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 1\n",
    "\n",
    "\n",
    "# Sample of statistics using nltk\n",
    "# Another option is defining a function and pass it as a parameter to FunctionTransformer\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "class LexicalStats (BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract lexical features from each document\"\"\"\n",
    "    \n",
    "    def number_sentences(self, doc):\n",
    "        sentences = sent_tokenize(doc, language='english')\n",
    "        return len(sentences)\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, docs):\n",
    "        return [{'length': len(doc),\n",
    "                 'num_sentences': self.number_sentences(doc)}\n",
    "                for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "def custom_tokenizer(words):\n",
    "    spanish_stopwords = stopwords.words('spanish')\n",
    "    \n",
    "    \"\"\"Preprocessing tokens as seen in the lexical notebook\"\"\"\n",
    "    tokens = word_tokenize(words.lower())\n",
    "    porter = PorterStemmer()\n",
    "    lemmas = [porter.stem(t) for t in tokens]\n",
    "    stoplist = spanish_stopwords\n",
    "    lemmas_clean = [w for w in lemmas if w not in stoplist]\n",
    "    punctuation = set(string.punctuation)\n",
    "    lemmas_punct = [w for w in lemmas_clean if  w not in punctuation]\n",
    "\n",
    "    \n",
    "    return lemmas_punct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simM import SimMatrix\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import feature_selection\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from gsitk.preprocess import normalize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Transformador (BaseEstimator, TransformerMixin):\n",
    "    def fit (self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        array = []\n",
    "        for i in range(0,len(X)):\n",
    "            array.append(normalize.preprocess(X[i]))\n",
    "        return np.array(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = KeyedVectors.load_word2vec_format('./SBW-vectors-300-min5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = pd.read_csv('ElhPolar_esV1.lex.txt', sep='\\t', header=None, names=['word', 'sentiment'])\n",
    "positive = lexicon[lexicon['sentiment']=='positive']['word'].values\n",
    "negative = lexicon[lexicon['sentiment']=='negative']['word'].values\n",
    "lexicon_words = [positive,negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "ngrams_featurizer = Pipeline([\n",
    "  ('count_vectorizer',  CountVectorizer(analyzer=\"word\", max_df=0.5, ngram_range=[1,2])),\n",
    "  ('tfidf_transformer', TfidfTransformer())\n",
    "])\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier #For Classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## All the steps of the Pipeline should end with a sparse vector as the input data\n",
    "\n",
    "pipeline = Pipeline([\n",
    "       ('features', FeatureUnion([\n",
    "                    ('words', TfidfVectorizer(tokenizer=custom_tokenizer)),\n",
    "\n",
    "                   ('ngrams', ngrams_featurizer),\n",
    "                   ('lexical_stats', Pipeline([\n",
    "                                        ('stats', LexicalStats()),\n",
    "                                        ('vectors', DictVectorizer())\n",
    "                                    ])),\n",
    "                   ('lda', Pipeline([ \n",
    "                            ('count', CountVectorizer(tokenizer=custom_tokenizer)),\n",
    "                            ('lda',  LatentDirichletAllocation(n_topics=4, max_iter=5,\n",
    "                                                   learning_method='online', \n",
    "                                                   learning_offset=50.,\n",
    "                                                   random_state=0))\n",
    "                        ])),\n",
    "                    ('emb', Pipeline([\n",
    "                            ('preprocess', Transformador()),\n",
    "                            ('simM', SimMatrix(lexicon_words, embedding=embeddings,\n",
    "                                               remove_stopwords=False, pooling=np.max,\n",
    "                                               weighting=False, n_lexicon_words=100,\n",
    "                                               lex_values=None)),\n",
    "                            ('scale', MinMaxScaler(feature_range=(0,2))),\n",
    "                            ('percent', feature_selection.SelectPercentile(feature_selection.f_classif, percentile=25)),\n",
    "                        ]))\n",
    "\n",
    "\n",
    "              ])),\n",
    "       \n",
    "\n",
    "                #('clf', MultinomialNB(alpha=.01))  # classifier\n",
    "            ('clf', SVC(C=10, gamma= 1, kernel='linear', probability=True))\n",
    "        #('clf', AdaBoostClassifier(n_estimators=50, base_estimator=MultinomialNB(alpha=.01), learning_rate=2))\n",
    "        #('modelknn', KNeighborsClassifier(n_neighbors = 13))\n",
    "    ])\n",
    "#SVC, KNeighborsClassifier, AdaBoostClassifier, MultinomialNB\n",
    "# Using KFold validation\n",
    "\n",
    "cv = KFold(X.shape[0], 4, shuffle=True, random_state=33)\n",
    "scores = cross_val_score(pipeline, X, y, cv=cv)\n",
    "print(\"Scores in every iteration\", scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tristeza']\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X,y)\n",
    "\n",
    "y=pipeline.predict(['Vaya partido mas aburrido'])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Felicidad']\n"
     ]
    }
   ],
   "source": [
    "z=pipeline.predict(['JAJAJJAAJJA'])\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# General import and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Training and test spliting\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Estimators\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Optimization\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "\n",
    "types_of_kernels = ['linear', 'rbf', 'poly']\n",
    "\n",
    "kernel = types_of_kernels[2]\n",
    "gamma = 3.0\n",
    "\n",
    "# Create kNN model\n",
    "model = SVC(kernel=kernel, probability=True, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emocion', 'noemocion']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords  \n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", stop_words='english', ngram_range=[1,6]) \n",
    "vectors = vectorizer.fit_transform(X)\n",
    "\n",
    "vectors2 = vectorizer.fit_transform(y)\n",
    "\n",
    "\n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(vectors, y, test_size=0.25, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This step will take some time \n",
    "# Train - This is not needed if you use K-Fold\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predicted = model.predict(X_test)\n",
    "expected = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.596858638743\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "gammas = np.logspace(-6, -1, 10, 30)\n",
    "\n",
    "\n",
    "gs = GridSearchCV(model, param_grid=dict(gamma=gammas))\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, y, test_size=0.25, random_state=33)\n",
    "gs.fit(X_train, y_train)\n",
    "scores = gs.score(X_test, y_test)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
