{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@AS_Manolete Y con el atleti podemos soñar otr...</td>\n",
       "      <td>Colera-Asco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>La 'rajada' de un ex objetivo del Barça sobre ...</td>\n",
       "      <td>Colera-Asco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@marcmarquez93 @3gerardpique @SergiRoberto10 @...</td>\n",
       "      <td>Colera-Asco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@LuisOmarTapia @IkerCasillas @ChampionsLeague ...</td>\n",
       "      <td>Colera-Asco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La \"rajada\" de un ex objetivo del Barça sobre ...</td>\n",
       "      <td>Colera-Asco</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet         Info\n",
       "0  @AS_Manolete Y con el atleti podemos soñar otr...  Colera-Asco\n",
       "1  La 'rajada' de un ex objetivo del Barça sobre ...  Colera-Asco\n",
       "2  @marcmarquez93 @3gerardpique @SergiRoberto10 @...  Colera-Asco\n",
       "3  @LuisOmarTapia @IkerCasillas @ChampionsLeague ...  Colera-Asco\n",
       "4  La \"rajada\" de un ex objetivo del Barça sobre ...  Colera-Asco"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The files are coded in ISO-8859-1\n",
    "\n",
    "df = pd.read_csv(\"data/Colera-No.csv\")\n",
    "df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and Y\n",
    "X = df['Tweet'].values.astype(str)\n",
    "y = df['Info'].values.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 1\n",
    "\n",
    "\n",
    "# Sample of statistics using nltk\n",
    "# Another option is defining a function and pass it as a parameter to FunctionTransformer\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "class LexicalStats (BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract lexical features from each document\"\"\"\n",
    "    \n",
    "    def number_sentences(self, doc):\n",
    "        sentences = sent_tokenize(doc, language='english')\n",
    "        return len(sentences)\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, docs):\n",
    "        return [{'length': len(doc),\n",
    "                 'num_sentences': self.number_sentences(doc)}\n",
    "                for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "def custom_tokenizer(words):\n",
    "    spanish_stopwords = stopwords.words('spanish')\n",
    "    \n",
    "    \"\"\"Preprocessing tokens as seen in the lexical notebook\"\"\"\n",
    "    tokens = word_tokenize(words.lower())\n",
    "    porter = PorterStemmer()\n",
    "    lemmas = [porter.stem(t) for t in tokens]\n",
    "    stoplist = spanish_stopwords\n",
    "    lemmas_clean = [w for w in lemmas if w not in stoplist]\n",
    "    punctuation = set(string.punctuation)\n",
    "    lemmas_punct = [w for w in lemmas_clean if  w not in punctuation]\n",
    "\n",
    "    \n",
    "    return lemmas_punct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simM import SimMatrix\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import feature_selection\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from gsitk.preprocess import normalize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Transformador (BaseEstimator, TransformerMixin):\n",
    "    def fit (self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        array = []\n",
    "        for i in range(0,len(X)):\n",
    "            array.append(normalize.preprocess(X[i]))\n",
    "        return np.array(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = KeyedVectors.load_word2vec_format('./SBW-vectors-300-min5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = pd.read_csv('ElhPolar_esV1.lex.txt', sep='\\t', header=None, names=['word', 'sentiment'])\n",
    "positive = lexicon[lexicon['sentiment']=='positive']['word'].values\n",
    "negative = lexicon[lexicon['sentiment']=='negative']['word'].values\n",
    "lexicon_words = [positive,negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores in every iteration [ 0.75      0.65625   0.6875    0.734375]\n",
      "Accuracy: 0.71 (+/- 0.07)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "\n",
    "ngrams_featurizer = Pipeline([\n",
    "  ('count_vectorizer',  CountVectorizer(analyzer=\"word\", max_df=0.5, ngram_range=[1,2])),\n",
    "  ('tfidf_transformer', TfidfTransformer())\n",
    "])\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier #For Classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "## All the steps of the Pipeline should end with a sparse vector as the input data\n",
    "\n",
    "pipeline = Pipeline([\n",
    "       ('features', FeatureUnion([\n",
    "                    ('words', TfidfVectorizer(tokenizer=custom_tokenizer)),\n",
    "\n",
    "                   ('ngrams', ngrams_featurizer),\n",
    "                   ('lexical_stats', Pipeline([\n",
    "                                        ('stats', LexicalStats()),\n",
    "                                        ('vectors', DictVectorizer())\n",
    "                                    ])),\n",
    "                   ('lda', Pipeline([ \n",
    "                            ('count', CountVectorizer(tokenizer=custom_tokenizer)),\n",
    "                            ('lda',  LatentDirichletAllocation(n_topics=4, max_iter=5,\n",
    "                                                   learning_method='online', \n",
    "                                                   learning_offset=50.,\n",
    "                                                   random_state=0))\n",
    "                        ])),\n",
    "                    ('emb', Pipeline([\n",
    "                            ('preprocess', Transformador()),\n",
    "                            ('simM', SimMatrix(lexicon_words, embedding=embeddings,\n",
    "                                               remove_stopwords=False, pooling=np.max,\n",
    "                                               weighting=False, n_lexicon_words=100,\n",
    "                                               lex_values=None)),\n",
    "                            ('scale', MinMaxScaler(feature_range=(0,2))),\n",
    "                            ('percent', feature_selection.SelectPercentile(feature_selection.f_classif, percentile=25)),\n",
    "                        ]))\n",
    "\n",
    "              ])),\n",
    "       \n",
    "\n",
    "                #('clf', MultinomialNB(alpha=.01))  # classifier\n",
    "            ('clf', SVC(C=10, gamma= 1, kernel='linear', probability=True))\n",
    "        #('clf', AdaBoostClassifier(n_estimators=50, base_estimator=MultinomialNB(alpha=.01), learning_rate=1))\n",
    "        #('modelknn', KNeighborsClassifier(n_neighbors = 13))\n",
    "    ])\n",
    "#SVC, KNeighborsClassifier, AdaBoostClassifier, MultinomialNB\n",
    "# Using KFold validation\n",
    "\n",
    "cv = KFold(X.shape[0], 4, shuffle=True, random_state=33)\n",
    "scores = cross_val_score(pipeline, X, y, cv=cv)\n",
    "print(\"Scores in every iteration\", scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
